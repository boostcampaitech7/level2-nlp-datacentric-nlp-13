{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. LLM으로 1차 분류\n",
    "- train.csv에서 자연스러운 문장과 부자연스러운 문장을 gemini로 분류\n",
    "- -> clean.csv, noise.csv 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "gemini 1.5 flash LLM을 이용하여 train.csv의 text가 자연스러운지 부자연스러운지 분류하고,\n",
    "자연스러운 문장은 clean.csv에, 부자연스러운 문장은 noise.csv에 저장하는 코드입니다.\n",
    "\"\"\"\n",
    "### LIBRARY ###\n",
    "import pandas as pd\n",
    "import os\n",
    "import time\n",
    "import re\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "import glob\n",
    "###############\n",
    "\n",
    "### SETTING ###\n",
    "API_KEY = ''\n",
    "INPUT_FILE = 'data/train.csv'\n",
    "###############\n",
    "\n",
    "# API 키 설정\n",
    "os.environ[\"GOOGLE_API_KEY\"] = API_KEY\n",
    "\n",
    "# 데이터 로드: 모든 행 읽기\n",
    "data_file = INPUT_FILE\n",
    "try:\n",
    "    data = pd.read_csv(data_file)\n",
    "    print(f\"총 {len(data)}개의 데이터를 로드했습니다.\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"파일을 찾을 수 없습니다: {data_file}\")\n",
    "    exit(1)\n",
    "except Exception as e:\n",
    "    print(f\"데이터 로드 중 오류 발생: {e}\")\n",
    "    exit(1)\n",
    "\n",
    "# 모델 초기화\n",
    "model = ChatGoogleGenerativeAI(model=\"gemini-1.5-flash-latest\")\n",
    "\n",
    "# 프롬프트 템플릿 정의\n",
    "prompt_template = PromptTemplate(\n",
    "    input_variables=[\"batch\"],\n",
    "    template=\"\"\"\n",
    "    다음은 뉴스 제목 형식의 텍스트 10개입니다:\n",
    "\n",
    "    {batch}\n",
    "\n",
    "    각 텍스트가 자연스러운지 부자연스러운지 판단해 주세요. \n",
    "\n",
    "    자연스러운 문장의 특징:\n",
    "    문법적으로 올바르고 의미가 명확합니다.\n",
    "    일반적인 뉴스 기사에서 사용되는 표현과 어휘를 사용합니다.\n",
    "    특별한 이상 문자나 오탈자가 없습니다.\n",
    "    문장의 대부분이 한글로 이루어져 있습니다.\n",
    "    \n",
    "    부자연스러운 문장의 특징:\n",
    "    문법 오류가 있거나 의미가 모호합니다.\n",
    "    이상한 문자, 기호, 알 수 없는 약어 등이 포함되어 있습니다.\n",
    "    읽기 어렵거나 이해하기 힘든 표현이 있습니다.\n",
    "\n",
    "    반드시 각 번호에 해당하는 텍스트의 분류 결과를 'natural' 또는 'unnatural'으로만 대답해 주세요.\n",
    "    응답은 번호 순서대로 한 줄에 하나씩 작성해 주세요.\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "# 결과를 저장할 리스트 초기화\n",
    "clean_results = []\n",
    "noise_results = []\n",
    "\n",
    "# 배치 크기 설정\n",
    "batch_size = 10\n",
    "total_rows = len(data)\n",
    "\n",
    "# 출력 파일명 설정\n",
    "clean_output_file = '1_clean.csv'\n",
    "noise_output_file = '2_noise.csv'\n",
    "\n",
    "# 기존 결과 파일이 존재하면 삭제하여 새로 생성\n",
    "if os.path.exists(clean_output_file):\n",
    "    os.remove(clean_output_file)\n",
    "    print(f\"{clean_output_file} 파일을 삭제하고 새로 생성합니다.\")\n",
    "if os.path.exists(noise_output_file):\n",
    "    os.remove(noise_output_file)\n",
    "    print(f\"{noise_output_file} 파일을 삭제하고 새로 생성합니다.\")\n",
    "\n",
    "# 배치 처리\n",
    "for start_idx in range(0, total_rows, batch_size):\n",
    "    end_idx = min(start_idx + batch_size, total_rows)\n",
    "    batch = data.iloc[start_idx:end_idx]\n",
    "\n",
    "    batch_texts = []\n",
    "    batch_ids = []\n",
    "    for i, (index, row) in enumerate(batch.iterrows(), 1):\n",
    "        id = row['ID']\n",
    "        text = row['text']\n",
    "        target = row['target']\n",
    "        batch_texts.append(f\"{i}. ID: {id}, Text: {text}\")\n",
    "        batch_ids.append(id)\n",
    "\n",
    "    batch_prompt = \"\\n\".join(batch_texts)\n",
    "\n",
    "    # 프롬프트 생성\n",
    "    prompt = prompt_template.format(batch=batch_prompt)\n",
    "\n",
    "    success = False\n",
    "    retry_count = 0\n",
    "    max_retries = 5\n",
    "\n",
    "    while not success and retry_count < max_retries:\n",
    "        try:\n",
    "            # 모델 예측\n",
    "            response = model.invoke(prompt)\n",
    "\n",
    "            # AIMessage 객체에서 실제 텍스트 추출\n",
    "            response_text = response.content.strip()\n",
    "            classifications = response_text.split('\\n')\n",
    "\n",
    "            if len(classifications) != len(batch_ids):\n",
    "                raise ValueError(\"응답된 분류 개수가 입력된 텍스트 개수와 일치하지 않습니다.\")\n",
    "\n",
    "            for idx, classification in enumerate(classifications):\n",
    "                id = batch_ids[idx]\n",
    "                text = batch.iloc[idx]['text']\n",
    "                target = batch.iloc[idx]['target']\n",
    "\n",
    "                # 번호와 점을 제거하여 실제 분류 결과 추출\n",
    "                match = re.match(r'\\d+\\.\\s*(.+)', classification)\n",
    "                if match:\n",
    "                    classification = match.group(1).strip()\n",
    "                else:\n",
    "                    classification = classification.strip()\n",
    "\n",
    "                if classification == 'natural':\n",
    "                    clean_results.append({\n",
    "                        \"ID\": id,\n",
    "                        \"text\": text,\n",
    "                        \"target\": target\n",
    "                    })\n",
    "                elif classification == 'unnatural':\n",
    "                    noise_results.append({\n",
    "                        \"ID\": id,\n",
    "                        \"text\": text,\n",
    "                        \"target\": target\n",
    "                    })\n",
    "                else:\n",
    "                    # 예기치 않은 응답은 부자연스러운 문장으로 처리\n",
    "                    noise_results.append({\n",
    "                        \"ID\": id,\n",
    "                        \"text\": text,\n",
    "                        \"target\": target\n",
    "                    })\n",
    "\n",
    "                print(f\"ID {id} 처리 완료: {classification}\")\n",
    "\n",
    "            success = True\n",
    "\n",
    "            time.sleep(2)  # request 사이에 지연\n",
    "\n",
    "        except Exception as e:\n",
    "            retry_count += 1\n",
    "            print(f\"배치 {start_idx + 1}-{end_idx} 처리 중 오류 발생: {e}\")\n",
    "            if 'ResourceExhausted' in str(e):\n",
    "                wait_time = 30  # 쿼터 초과 시 30초 대기\n",
    "                print(f\"{wait_time}초 후에 재시도합니다.\")\n",
    "                time.sleep(wait_time)\n",
    "            else:\n",
    "                wait_time = 10  # 기타 오류 시 10초 대기\n",
    "                print(f\"{wait_time}초 후에 재시도합니다.\")\n",
    "                time.sleep(wait_time)\n",
    "\n",
    "    if not success:\n",
    "        print(f\"배치 {start_idx + 1}-{end_idx} 처리를 건너뜁니다. 부자연스러운 문장으로 저장합니다.\")\n",
    "        for id in batch_ids:\n",
    "            text = data.loc[data['ID'] == id, 'text'].values[0]\n",
    "            target = data.loc[data['ID'] == id, 'target'].values[0]\n",
    "            noise_results.append({\n",
    "                \"ID\": id,\n",
    "                \"text\": text,\n",
    "                \"target\": target\n",
    "            })\n",
    "            print(f\"ID {id} 처리를 실패하여 부자연스러운 문장으로 저장했습니다.\")\n",
    "\n",
    "    # clean.csv 저장\n",
    "    if clean_results:\n",
    "        clean_df = pd.DataFrame(clean_results)\n",
    "        if start_idx == 0:\n",
    "            clean_df.to_csv(clean_output_file, index=False, encoding='utf-8-sig')\n",
    "        else:\n",
    "            clean_df.to_csv(clean_output_file, mode='a', index=False, header=False, encoding='utf-8-sig')\n",
    "        print(f\"Clean 데이터 저장 완료: {len(clean_results)}개\")\n",
    "        clean_results = []\n",
    "    else:\n",
    "        print(\"Clean 결과가 없습니다. clean.csv를 저장하지 않습니다.\")\n",
    "\n",
    "    # noise.csv 저장\n",
    "    if noise_results:\n",
    "        noise_df = pd.DataFrame(noise_results)\n",
    "        if start_idx == 0:\n",
    "            noise_df.to_csv(noise_output_file, index=False, encoding='utf-8-sig')\n",
    "        else:\n",
    "            noise_df.to_csv(noise_output_file, mode='a', index=False, header=False, encoding='utf-8-sig')\n",
    "        print(f\"Noise 데이터 저장 완료: {len(noise_results)}개\")\n",
    "        noise_results = []\n",
    "    else:\n",
    "        print(\"Noise 결과가 없습니다. noise.csv를 저장하지 않습니다.\")\n",
    "\n",
    "    print(f\"{end_idx}/{total_rows}개 데이터 처리 완료. 중간 결과를 저장했습니다.\")\n",
    "\n",
    "print(\"모든 데이터 처리가 완료되었습니다.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. 특수문자 비율을 계산해 재분류할 행들을 필터링\n",
    "- 1에서 생성된 clean.csv, noise.csv를 살펴보니, 특수문자 비율 0.3을 기준으로 오분류된 데이터가 많음\n",
    "- -> filtered.csv에 저장한 뒤 재분류"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_special_char_ratio(text):\n",
    "    if len(text) == 0:\n",
    "        return 0\n",
    "    special_chars = re.findall(r'[^가-힣\\s…·.%美北中朴日靑∼↑英與↓]', str(text)) # 한글, 공백, test data에서 많이 쓰인 상위 15개 문자는 특수문자 비율 계산에서 제외\n",
    "    return round((len(special_chars) / len(text)), 3)\n",
    "\n",
    "clean = pd.read_csv('1_clean.csv')\n",
    "noise = pd.read_csv('1_noise.csv')\n",
    "\n",
    "clean['ratio'] = clean['text'].apply(calculate_special_char_ratio)\n",
    "noise['ratio'] = noise['text'].apply(calculate_special_char_ratio)\n",
    "clean_filtered = clean[clean['ratio'] > 0.3]\n",
    "noise_filtered = noise[noise['ratio'] < 0.3]\n",
    "\n",
    "filtered = pd.concat([clean_filtered, noise_filtered])\n",
    "filtered.to_csv('2_filtered.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. LLM으로 2차 분류\n",
    "- 2에서 생성된 filtered.csv를 재분류\n",
    "1. filtered.csv를 보니 특수문자 비율이 0.13 미만인 행은 전부 clean함 -> 'natural'로 라벨링하고 filtered_clean에 저장\n",
    "2. 나머지는 LLM으로 재분류\n",
    "- -> filtered_clean, filtered_noise, filtered_labeled.csv 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "filtered.csv 파일의 데이터를 gemini 1.5 flash LLM을 사용하여 재분류하고,\n",
    "is_natural 라벨을 추가하여 filtered_labeled.csv에 저장하는 코드입니다.\n",
    "\"\"\"\n",
    "\n",
    "### SETTING ###\n",
    "API_KEY = ''\n",
    "INPUT_FILE = '2_filtered.csv'\n",
    "###############\n",
    "\n",
    "# API 키 설정\n",
    "os.environ[\"GOOGLE_API_KEY\"] = API_KEY\n",
    "\n",
    "# 데이터 로드\n",
    "data_file = INPUT_FILE\n",
    "try:\n",
    "    data = pd.read_csv(data_file)\n",
    "    print(f\"총 {len(data)}개의 데이터를 로드했습니다.\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"파일을 찾을 수 없습니다: {data_file}\")\n",
    "    exit(1)\n",
    "except Exception as e:\n",
    "    print(f\"데이터 로드 중 오류 발생: {e}\")\n",
    "    exit(1)\n",
    "\n",
    "# is_natural 열 추가 및 초기화\n",
    "data['is_natural'] = None\n",
    "\n",
    "# ratio가 natural_threshold 미만인 행은 'natural'으로 라벨링\n",
    "natural_threshold = 0.13\n",
    "data.loc[data['ratio'] < natural_threshold, 'is_natural'] = 'natural'\n",
    "\n",
    "# ratio가 natural_threshold 미만인 행들을 별도로 3_filtered_clean.csv에 저장\n",
    "clean_initial = data[data['ratio'] < natural_threshold][['ID', 'text', 'target']]\n",
    "clean_initial.to_csv('3_filtered_clean.csv', index=False, encoding='utf-8-sig')\n",
    "print(f\"ratio가 {natural_threshold} 미만인 {len(clean_initial)}개의 행을 3_filtered_clean.csv에 저장했습니다.\")\n",
    "\n",
    "# ratio가 natural_threshold 이상인 행들만 재분류 대상\n",
    "reclassify_data = data[data['ratio'] >= natural_threshold].copy()\n",
    "print(f\"재분류 대상 데이터 수: {len(reclassify_data)}개\")\n",
    "\n",
    "if reclassify_data.empty:\n",
    "    print(\"재분류할 데이터가 없습니다.\")\n",
    "else:\n",
    "    # 모델 초기화\n",
    "    model = ChatGoogleGenerativeAI(model=\"gemini-1.5-flash-latest\")\n",
    "\n",
    "    # 프롬프트 템플릿 정의\n",
    "    prompt_template = PromptTemplate(\n",
    "        input_variables=[\"batch\"],\n",
    "        template=\"\"\"\n",
    "        다음은 뉴스 제목 형식의 텍스트 10개입니다:\n",
    "\n",
    "        {batch}\n",
    "\n",
    "        각 텍스트가 자연스러운지 부자연스러운지 판단해 주세요. \n",
    "        자연스러운 문장은 문법적으로 올바르고, 특수문자나 무작위 문자의 삽입 없이 의미가 명확하게 전달되는 문장입니다.\n",
    "        부자연스러운 문장은 단어 중간에 특수문자, 알파벳, 숫자가 무작위로 삽입되어 있거나 단어가 왜곡되어 문맥 파악이 어려운 문장입니다.\n",
    "        반드시 각 번호에 해당하는 텍스트의 분류 결과를 'natural' 또는 'unnatural'으로만 대답해 주세요.\n",
    "        응답은 번호 순서대로 한 줄에 하나씩 작성해 주세요.\n",
    "        \"\"\"\n",
    "    )\n",
    "\n",
    "    # 결과를 저장할 리스트 초기화\n",
    "    filtered_clean_results = []\n",
    "    filtered_noise_results = []\n",
    "\n",
    "    # 배치 크기 설정\n",
    "    batch_size = 10\n",
    "    total_rows = len(reclassify_data)\n",
    "\n",
    "    # 출력 파일명 설정\n",
    "    filtered_clean_output_file = '3_filtered_clean.csv'\n",
    "    filtered_noise_output_file = '3_filtered_noise.csv'\n",
    "\n",
    "    # 기존 결과 파일이 존재하면 삭제하여 새로 생성 (clean_initial은 이미 저장했으므로 여기서는 삭제하지 않음)\n",
    "    if os.path.exists(filtered_noise_output_file):\n",
    "        os.remove(filtered_noise_output_file)\n",
    "        print(f\"{filtered_noise_output_file} 파일을 삭제하고 새로 생성합니다.\")\n",
    "\n",
    "    # 배치 처리\n",
    "    for start_idx in range(0, total_rows, batch_size):\n",
    "        end_idx = min(start_idx + batch_size, total_rows)\n",
    "        batch = reclassify_data.iloc[start_idx:end_idx]\n",
    "\n",
    "        # Prepare batch text\n",
    "        batch_texts = []\n",
    "        batch_ids = []\n",
    "        for i, (index, row) in enumerate(batch.iterrows(), 1):\n",
    "            id = row['ID']\n",
    "            text = row['text']\n",
    "            batch_texts.append(f\"{i}. ID: {id}, Text: {text}\")\n",
    "            batch_ids.append(id)\n",
    "\n",
    "        batch_prompt = \"\\n\".join(batch_texts)\n",
    "\n",
    "        # 프롬프트 생성\n",
    "        prompt = prompt_template.format(batch=batch_prompt)\n",
    "\n",
    "        success = False\n",
    "        retry_count = 0\n",
    "        max_retries = 5\n",
    "\n",
    "        while not success and retry_count < max_retries:\n",
    "            try:\n",
    "                # 모델 예측\n",
    "                response = model.invoke(prompt)\n",
    "\n",
    "                # AIMessage 객체에서 실제 텍스트 추출\n",
    "                response_text = response.content.strip()\n",
    "                classifications = response_text.split('\\n')\n",
    "\n",
    "                if len(classifications) != len(batch_ids):\n",
    "                    raise ValueError(\"응답된 분류 개수가 입력된 텍스트 개수와 일치하지 않습니다.\")\n",
    "\n",
    "                for idx, classification in enumerate(classifications):\n",
    "                    id = batch_ids[idx]\n",
    "                    text = batch.iloc[idx]['text']\n",
    "                    target = batch.iloc[idx]['target']\n",
    "\n",
    "                    # 번호와 점을 제거하여 실제 분류 결과 추출\n",
    "                    match = re.match(r'\\d+\\.\\s*(.+)', classification)\n",
    "                    if match:\n",
    "                        classification = match.group(1).strip()\n",
    "                    else:\n",
    "                        classification = classification.strip()\n",
    "\n",
    "                    if classification.lower() == 'natural':\n",
    "                        label = 'natural'\n",
    "                        filtered_clean_results.append({\n",
    "                            \"ID\": id,\n",
    "                            \"text\": text,\n",
    "                            \"target\": target,\n",
    "                            \"label\": label\n",
    "                        })\n",
    "                    elif classification.lower() == 'unnatural':\n",
    "                        label = 'unnatural'\n",
    "                        filtered_noise_results.append({\n",
    "                            \"ID\": id,\n",
    "                            \"text\": text,\n",
    "                            \"target\": target,\n",
    "                            \"label\": label\n",
    "                        })\n",
    "                    else:\n",
    "                        # 예기치 않은 응답은 'unnatural'로 처리\n",
    "                        label = 'unnatural'\n",
    "                        filtered_noise_results.append({\n",
    "                            \"ID\": id,\n",
    "                            \"text\": text,\n",
    "                            \"target\": target,\n",
    "                            \"label\": label\n",
    "                        })\n",
    "\n",
    "                    # is_natural 컬럼 업데이트\n",
    "                    data.loc[data['ID'] == id, 'is_natural'] = label\n",
    "\n",
    "                    print(f\"ID {id} 처리 완료: {label}\")\n",
    "\n",
    "                success = True\n",
    "\n",
    "                # 요청 사이에 지연 시간 추가\n",
    "                time.sleep(2)  # 지연 시간을 늘려서 API 부하를 줄입니다\n",
    "\n",
    "            except Exception as e:\n",
    "                retry_count += 1\n",
    "                print(f\"배치 {start_idx + 1}-{end_idx} 처리 중 오류 발생: {e}\")\n",
    "                if 'ResourceExhausted' in str(e):\n",
    "                    wait_time = 30  # 쿼터 초과 시 30초 대기\n",
    "                    print(f\"{wait_time}초 후에 재시도합니다.\")\n",
    "                    time.sleep(wait_time)\n",
    "                else:\n",
    "                    wait_time = 10  # 기타 오류 시 10초 대기\n",
    "                    print(f\"{wait_time}초 후에 재시도합니다.\")\n",
    "                    time.sleep(wait_time)\n",
    "\n",
    "        if not success:\n",
    "            print(f\"배치 {start_idx + 1}-{end_idx} 처리를 건너뜁니다. 'unnatural'으로 라벨링합니다.\")\n",
    "            for idx in range(len(batch)):\n",
    "                id = batch.iloc[idx]['ID']\n",
    "                text = batch.iloc[idx]['text']\n",
    "                target = batch.iloc[idx]['target']\n",
    "                label = 'unnatural'\n",
    "                filtered_noise_results.append({\n",
    "                    \"ID\": id,\n",
    "                    \"text\": text,\n",
    "                    \"target\": target,\n",
    "                    \"label\": label\n",
    "                })\n",
    "                # is_natural 컬럼 업데이트\n",
    "                data.loc[data['ID'] == id, 'is_natural'] = label\n",
    "                print(f\"ID {id} 처리를 실패하여 'unnatural'으로 라벨링했습니다.\")\n",
    "\n",
    "        # 배치 처리 후 결과 저장\n",
    "        # filtered_clean.csv 저장 (이미 초기 clean을 저장했으므로 append)\n",
    "        if filtered_clean_results:\n",
    "            filtered_clean_df = pd.DataFrame(filtered_clean_results)\n",
    "            # 헤더는 첫 번째 배치에서만 추가\n",
    "            header = not os.path.exists(filtered_clean_output_file)\n",
    "            filtered_clean_df.to_csv(filtered_clean_output_file, mode='a', index=False, header=header, encoding='utf-8-sig')\n",
    "            print(f\"Filtered Clean 데이터 저장 완료: {len(filtered_clean_results)}개\")\n",
    "            filtered_clean_results = []  # 리스트 초기화\n",
    "        else:\n",
    "            print(\"Filtered Clean 결과가 없습니다. filtered_clean.csv를 저장하지 않습니다.\")\n",
    "\n",
    "        # filtered_noise.csv 저장\n",
    "        if filtered_noise_results:\n",
    "            filtered_noise_df = pd.DataFrame(filtered_noise_results)\n",
    "            # 헤더는 첫 번째 배치에서만 추가\n",
    "            header = not os.path.exists(filtered_noise_output_file)\n",
    "            filtered_noise_df.to_csv(filtered_noise_output_file, mode='a', index=False, header=header, encoding='utf-8-sig')\n",
    "            print(f\"Filtered Noise 데이터 저장 완료: {len(filtered_noise_results)}개\")\n",
    "            filtered_noise_results = []  # 리스트 초기화\n",
    "        else:\n",
    "            print(\"Filtered Noise 결과가 없습니다. filtered_noise.csv를 저장하지 않습니다.\")\n",
    "\n",
    "        print(f\"{end_idx}/{total_rows}개 데이터 처리 완료. 중간 결과를 저장했습니다.\")\n",
    "\n",
    "    # 최종 결과 저장\n",
    "    # 필요한 열만 선택하여 filtered_labeled.csv로 저장\n",
    "    final_df = data[['ID', 'text', 'target', 'is_natural']].copy()\n",
    "    final_df.rename(columns={'is_natural': 'label'}, inplace=True)\n",
    "    final_df.to_csv('3_filtered_labeled.csv', index=False, encoding='utf-8-sig')\n",
    "    print(\"filtered_labeled.csv 파일이 성공적으로 저장되었습니다.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. 2차 분류한 결과를 반영\n",
    "- -> clean_again.csv, noise_again.csv 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "clean.csv와 noise.csv 파일을 불러와 filtered_labeled.csv의 라벨에 따라 데이터를 재분류하고,\n",
    "최종적으로 clean_again.csv와 noise_again.csv 파일로 저장하는 스크립트입니다.\n",
    "\"\"\"\n",
    "# 파일 경로 설정\n",
    "clean_file = '1_clean.csv'\n",
    "noise_file = '1_noise.csv'\n",
    "filtered_labeled_file = '3_filtered_labeled.csv'\n",
    "clean_again_file = '4_clean_again.csv'\n",
    "noise_again_file = '4_noise_again.csv'\n",
    "\n",
    "# 데이터 로드\n",
    "try:\n",
    "    clean_df = pd.read_csv(clean_file)\n",
    "    print(f\"'{clean_file}' 파일을 성공적으로 로드했습니다. 총 {len(clean_df)}개의 행.\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"'{clean_file}' 파일을 찾을 수 없습니다. 빈 DataFrame을 생성합니다.\")\n",
    "    clean_df = pd.DataFrame(columns=['ID', 'text', 'target'])\n",
    "\n",
    "try:\n",
    "    noise_df = pd.read_csv(noise_file)\n",
    "    print(f\"'{noise_file}' 파일을 성공적으로 로드했습니다. 총 {len(noise_df)}개의 행.\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"'{noise_file}' 파일을 찾을 수 없습니다. 빈 DataFrame을 생성합니다.\")\n",
    "    noise_df = pd.DataFrame(columns=['ID', 'text', 'target'])\n",
    "\n",
    "try:\n",
    "    filtered_labeled_df = pd.read_csv(filtered_labeled_file)\n",
    "    print(f\"'{filtered_labeled_file}' 파일을 성공적으로 로드했습니다. 총 {len(filtered_labeled_df)}개의 행.\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"'{filtered_labeled_file}' 파일을 찾을 수 없습니다. 스크립트를 종료합니다.\")\n",
    "    exit(1)\n",
    "\n",
    "# 'natural' 라벨인 데이터 처리\n",
    "natural_df = filtered_labeled_df[filtered_labeled_df['label'] == 'natural']\n",
    "print(f\"'natural' 라벨인 데이터 수: {len(natural_df)}개.\")\n",
    "\n",
    "# 'natural' 라벨인 데이터 중 noise.csv에 있는 행 찾기\n",
    "natural_in_noise = natural_df[natural_df['ID'].isin(noise_df['ID'])]\n",
    "print(f\"'natural' 라벨인 데이터 중 noise.csv에 있는 행 수: {len(natural_in_noise)}개.\")\n",
    "\n",
    "# noise.csv에서 해당 행 삭제\n",
    "if not natural_in_noise.empty:\n",
    "    noise_df = noise_df[~noise_df['ID'].isin(natural_in_noise['ID'])]\n",
    "    print(f\"noise.csv에서 {len(natural_in_noise)}개의 행을 삭제했습니다.\")\n",
    "\n",
    "    # clean.csv에 해당 행 추가 (중복 방지)\n",
    "    clean_df = pd.concat([clean_df, natural_in_noise[['ID', 'text', 'target']]], ignore_index=True)\n",
    "    clean_df.drop_duplicates(subset=['ID'], inplace=True)\n",
    "    print(f\"clean.csv에 {len(natural_in_noise)}개의 행을 추가했습니다.\")\n",
    "else:\n",
    "    print(\"재분류할 'natural' 라벨의 행이 noise.csv에 존재하지 않습니다.\")\n",
    "\n",
    "# 'unnatural' 라벨인 데이터 처리\n",
    "unnatural_df = filtered_labeled_df[filtered_labeled_df['label'] == 'unnatural']\n",
    "print(f\"'unnatural' 라벨인 데이터 수: {len(unnatural_df)}개.\")\n",
    "\n",
    "# 'unnatural' 라벨인 데이터 중 clean.csv에 있는 행 찾기\n",
    "unnatural_in_clean = unnatural_df[unnatural_df['ID'].isin(clean_df['ID'])]\n",
    "print(f\"'unnatural' 라벨인 데이터 중 clean.csv에 있는 행 수: {len(unnatural_in_clean)}개.\")\n",
    "\n",
    "# clean.csv에서 해당 행 삭제\n",
    "if not unnatural_in_clean.empty:\n",
    "    clean_df = clean_df[~clean_df['ID'].isin(unnatural_in_clean['ID'])]\n",
    "    print(f\"clean.csv에서 {len(unnatural_in_clean)}개의 행을 삭제했습니다.\")\n",
    "\n",
    "    # noise.csv에 해당 행 추가 (중복 방지)\n",
    "    noise_df = pd.concat([noise_df, unnatural_in_clean[['ID', 'text', 'target']]], ignore_index=True)\n",
    "    noise_df.drop_duplicates(subset=['ID'], inplace=True)\n",
    "    print(f\"noise.csv에 {len(unnatural_in_clean)}개의 행을 추가했습니다.\")\n",
    "else:\n",
    "    print(\"재분류할 'unnatural' 라벨의 행이 clean.csv에 존재하지 않습니다.\")\n",
    "\n",
    "# 최종 DataFrame 저장\n",
    "# clean_again.csv 저장\n",
    "clean_again_df = clean_df[['ID', 'text', 'target']]\n",
    "clean_again_df.to_csv(clean_again_file, index=False, encoding='utf-8-sig')\n",
    "print(f\"최종 clean_again.csv 파일을 저장했습니다. 총 {len(clean_again_df)}개의 행.\")\n",
    "\n",
    "# noise_again.csv 저장\n",
    "noise_again_df = noise_df[['ID', 'text', 'target']]\n",
    "noise_again_df.to_csv(noise_again_file, index=False, encoding='utf-8-sig')\n",
    "print(f\"최종 noise_again.csv 파일을 저장했습니다. 총 {len(noise_again_df)}개의 행.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. LLM으로 3차 분류\n",
    "- clean_again.csv에 오분류된 데이터를 보니 대부분 특수문자 비율이 0.13~0.29임. -> LLM으로 한번 더 분류\n",
    "- -> 최종 분류물인 label_noise, text_noise 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "4_clean_again.csv의 text의 ratio가 0.13 초과 0.29 미만인 행을 추출해서\n",
    "gemini로 자연스러운지 부자연스러운지 여부를 검사한 뒤\n",
    "자연스러운 데이터는 기존의 4_clean_again.csv와 합쳐 label_noise.csv로 저장하고, (ID, text, target으로 구성)\n",
    "부자연스러운 데이터는 기존의 4_noise_again.csv와 합쳐 text_noise.csv로 저장합니다. (ID, text, target으로 구성)\n",
    "저장할 때 label_noise.csv, text_noise.csv 각각의 전체 데이터 수를 출력합니다.\n",
    "\"\"\"\n",
    "\n",
    "# API key 설정\n",
    "API_KEY = ''\n",
    "os.environ[\"GOOGLE_API_KEY\"] = API_KEY\n",
    "\n",
    "# 파일 경로 설정\n",
    "clean_again_file = '4_clean_again.csv'\n",
    "noise_again_file = '4_noise_again.csv'\n",
    "label_noise_file = 'split_train_data/label_noise.csv'\n",
    "text_noise_file = 'split_train_data/text_noise.csv'\n",
    "\n",
    "# 데이터 로드\n",
    "try:\n",
    "    clean_df = pd.read_csv(clean_again_file)\n",
    "    print(f\"'{clean_again_file}' 파일을 성공적으로 로드했습니다. 총 {len(clean_df)}개의 행.\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"'{clean_again_file}' 파일을 찾을 수 없습니다. 스크립트를 종료합니다.\")\n",
    "    exit(1)\n",
    "\n",
    "try:\n",
    "    noise_df = pd.read_csv(noise_again_file)\n",
    "    print(f\"'{noise_again_file}' 파일을 성공적으로 로드했습니다. 총 {len(noise_df)}개의 행.\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"'{noise_again_file}' 파일을 찾을 수 없습니다. 스크립트를 종료합니다.\")\n",
    "    exit(1)\n",
    "\n",
    "# ratio 계산 함수 정의\n",
    "def calculate_special_char_ratio(text):\n",
    "    # 텍스트 길이가 0인 경우를 처리\n",
    "    if len(text) == 0:\n",
    "        return 0\n",
    "    # 특수문자 비율 계산\n",
    "    special_chars = re.findall(r'[^가-힣\\s…·.%美北中朴日靑∼↑英與↓]', str(text))  # 한글과 공백, 일부 한자 및 기호를 제외한 모든 문자\n",
    "    return round((len(special_chars) / len(text)), 3)\n",
    "\n",
    "# ratio 계산\n",
    "clean_df['ratio'] = clean_df['text'].apply(calculate_special_char_ratio)\n",
    "\n",
    "# 조건에 맞는 행 추출\n",
    "subset_df = clean_df[(clean_df['ratio'] > 0.13) & (clean_df['ratio'] < 0.29)]\n",
    "print(f\"검사 대상 데이터 수: {len(subset_df)}개\")\n",
    "\n",
    "if subset_df.empty:\n",
    "    print(\"검사 대상 데이터가 없습니다. 기존 데이터를 그대로 저장합니다.\")\n",
    "    # 기존 데이터를 그대로 저장\n",
    "    clean_df[['ID', 'text', 'target']].to_csv(label_noise_file, index=False, encoding='utf-8-sig')\n",
    "    noise_df.to_csv(text_noise_file, index=False, encoding='utf-8-sig')\n",
    "    print(f\"최종 label_noise.csv 파일을 저장했습니다. 총 {len(clean_df)}개의 행.\")\n",
    "    print(f\"최종 text_noise.csv 파일을 저장했습니다. 총 {len(noise_df)}개의 행.\")\n",
    "else:\n",
    "    # Gemini 모델 초기화\n",
    "    model = ChatGoogleGenerativeAI(model=\"gemini-1.5-flash-latest\")\n",
    "\n",
    "    # 프롬프트 템플릿 정의\n",
    "    prompt_template = PromptTemplate(\n",
    "        input_variables=[\"batch\"],\n",
    "        template=\"\"\"\n",
    "        다음은 뉴스 제목 형식의 텍스트 10개입니다:\n",
    "\n",
    "        {batch}\n",
    "\n",
    "        각 텍스트가 자연스러운지 부자연스러운지 판단해 주세요. \n",
    "        자연스러운 문장은 문법적으로 올바르고, 특수문자나 무작위 문자의 삽입 없이 의미가 명확하게 전달되는 문장입니다.\n",
    "        부자연스러운 문장은 단어 중간에 특수문자, 알파벳, 숫자가 무작위로 삽입되어 있거나 단어가 왜곡되어 문맥 파악이 어려운 문장입니다.\n",
    "        반드시 각 번호에 해당하는 텍스트의 분류 결과를 'natural' 또는 'unnatural'으로만 대답해 주세요.\n",
    "        응답은 번호 순서대로 한 줄에 하나씩 작성해 주세요.\n",
    "        \"\"\"\n",
    "    )\n",
    "\n",
    "    # 검사 대상 데이터로부터 필요한 컬럼 추출\n",
    "    classify_df = subset_df[['ID', 'text', 'target']].copy()\n",
    "\n",
    "    # 결과를 저장할 리스트 초기화\n",
    "    natural_results = []\n",
    "    unnatural_results = []\n",
    "\n",
    "    # 배치 크기 설정\n",
    "    batch_size = 10\n",
    "    total_rows = len(classify_df)\n",
    "\n",
    "    # 배치 처리\n",
    "    for start_idx in range(0, total_rows, batch_size):\n",
    "        end_idx = min(start_idx + batch_size, total_rows)\n",
    "        batch = classify_df.iloc[start_idx:end_idx]\n",
    "\n",
    "        # Prepare batch text\n",
    "        batch_texts = []\n",
    "        batch_ids = []\n",
    "        for i, (index, row) in enumerate(batch.iterrows(), 1):\n",
    "            id = row['ID']\n",
    "            text = row['text']\n",
    "            batch_texts.append(f\"{i}. ID: {id}, Text: {text}\")\n",
    "            batch_ids.append(id)\n",
    "\n",
    "        batch_prompt = \"\\n\".join(batch_texts)\n",
    "\n",
    "        # 프롬프트 생성\n",
    "        prompt = prompt_template.format(batch=batch_prompt)\n",
    "\n",
    "        success = False\n",
    "        retry_count = 0\n",
    "        max_retries = 5\n",
    "\n",
    "        while not success and retry_count < max_retries:\n",
    "            try:\n",
    "                # 모델 예측\n",
    "                response = model.invoke(prompt)\n",
    "\n",
    "                # AIMessage 객체에서 실제 텍스트 추출\n",
    "                response_text = response.content.strip()\n",
    "                classifications = response_text.split('\\n')\n",
    "\n",
    "                if len(classifications) != len(batch_ids):\n",
    "                    raise ValueError(\"응답된 분류 개수가 입력된 텍스트 개수와 일치하지 않습니다.\")\n",
    "\n",
    "                for idx, classification in enumerate(classifications):\n",
    "                    id = batch_ids[idx]\n",
    "                    text = batch.iloc[idx]['text']\n",
    "                    target = batch.iloc[idx]['target']\n",
    "\n",
    "                    # 번호와 점을 제거하여 실제 분류 결과 추출\n",
    "                    match = re.match(r'\\d+\\.\\s*(.+)', classification)\n",
    "                    if match:\n",
    "                        classification = match.group(1).strip()\n",
    "                    else:\n",
    "                        classification = classification.strip()\n",
    "\n",
    "                    if classification.lower() == 'natural':\n",
    "                        natural_results.append({\n",
    "                            \"ID\": id,\n",
    "                            \"text\": text,\n",
    "                            \"target\": target\n",
    "                        })\n",
    "                    elif classification.lower() == 'unnatural':\n",
    "                        unnatural_results.append({\n",
    "                            \"ID\": id,\n",
    "                            \"text\": text,\n",
    "                            \"target\": target\n",
    "                        })\n",
    "                    else:\n",
    "                        # 예기치 않은 응답은 'unnatural'로 처리\n",
    "                        unnatural_results.append({\n",
    "                            \"ID\": id,\n",
    "                            \"text\": text,\n",
    "                            \"target\": target\n",
    "                        })\n",
    "\n",
    "                    print(f\"ID {id} 처리 완료: {classification}\")\n",
    "\n",
    "                success = True\n",
    "\n",
    "                # 요청 사이에 지연 시간 추가\n",
    "                time.sleep(2)  # 지연 시간을 늘려서 API 부하를 줄입니다\n",
    "\n",
    "            except Exception as e:\n",
    "                retry_count += 1\n",
    "                print(f\"배치 {start_idx + 1}-{end_idx} 처리 중 오류 발생: {e}\")\n",
    "                if 'ResourceExhausted' in str(e):\n",
    "                    wait_time = 30  # 쿼터 초과 시 30초 대기\n",
    "                    print(f\"{wait_time}초 후에 재시도합니다.\")\n",
    "                    time.sleep(wait_time)\n",
    "                else:\n",
    "                    wait_time = 10  # 기타 오류 시 10초 대기\n",
    "                    print(f\"{wait_time}초 후에 재시도합니다.\")\n",
    "                    time.sleep(wait_time)\n",
    "\n",
    "        if not success:\n",
    "            print(f\"배치 {start_idx + 1}-{end_idx} 처리를 건너뜁니다. 'unnatural'로 처리합니다.\")\n",
    "            for idx in range(len(batch)):\n",
    "                id = batch_ids[idx]\n",
    "                text = batch.iloc[idx]['text']\n",
    "                target = batch.iloc[idx]['target']\n",
    "                unnatural_results.append({\n",
    "                    \"ID\": id,\n",
    "                    \"text\": text,\n",
    "                    \"target\": target\n",
    "                })\n",
    "                print(f\"ID {id} 처리를 실패하여 'unnatural'로 처리했습니다.\")\n",
    "\n",
    "    # 검사 대상에서 제외된 clean_df의 나머지 데이터 추출\n",
    "    remaining_clean_df = clean_df[~clean_df['ID'].isin(subset_df['ID'])][['ID', 'text', 'target']]\n",
    "\n",
    "    # 자연스러운 데이터 합치기\n",
    "    natural_df = pd.DataFrame(natural_results)\n",
    "    final_clean_df = pd.concat([remaining_clean_df, natural_df], ignore_index=True)\n",
    "    final_clean_df.drop_duplicates(subset=['ID'], inplace=True)\n",
    "\n",
    "    # 부자연스러운 데이터 noise_df에 추가\n",
    "    unnatural_df = pd.DataFrame(unnatural_results)\n",
    "    final_noise_df = pd.concat([noise_df, unnatural_df], ignore_index=True)\n",
    "    final_noise_df.drop_duplicates(subset=['ID'], inplace=True)\n",
    "\n",
    "    # 최종 데이터 저장\n",
    "    final_clean_df.to_csv(label_noise_file, index=False, encoding='utf-8-sig')\n",
    "    final_noise_df.to_csv(text_noise_file, index=False, encoding='utf-8-sig')\n",
    "\n",
    "    print(f\"최종 label_noise.csv 파일을 저장했습니다. 총 {len(final_clean_df)}개의 행.\")\n",
    "    print(f\"최종 text_noise.csv 파일을 저장했습니다. 총 {len(final_noise_df)}개의 행.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6. split 과정 중 생긴 파일들 삭제"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 현재 디렉토리 내에 있는 모든 .csv 파일 찾기\n",
    "csv_files = glob.glob(\"*.csv\")\n",
    "\n",
    "# 각 .csv 파일 삭제\n",
    "for file in csv_files:\n",
    "    try:\n",
    "        os.remove(file)\n",
    "        print(f\"삭제: {file}\")\n",
    "    except Exception as e:\n",
    "        print(f\"삭제 오류 {file}: {e}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
