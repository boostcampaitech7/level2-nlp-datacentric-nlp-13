{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. text cleaning.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "gemini 1.5 flash LLM을 이용하여, filtering 결과로 생성된 text_noise.csv의 text를 정제하는 코드입니다.\n",
    "정제 결과는 text_cleaned_raw.csv에 저장되며, postprocess.py를 통해 후처리해야 합니다.\n",
    "\"\"\"\n",
    "\n",
    "INPUT_FILE = \"../1_filtering/5_text_noise.csv\"\n",
    "OUTPUT_FILE = \"text_cleaned_raw.csv\"\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "import time\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "# API 키 설정\n",
    "os.environ[\"GOOGLE_API_KEY\"] = \" \"\n",
    "\n",
    "# 데이터 로드\n",
    "data = pd.read_csv(INPUT_FILE)\n",
    "\n",
    "# 모델 초기화\n",
    "model = ChatGoogleGenerativeAI(model=\"gemini-1.5-flash-latest\")\n",
    "\n",
    "# 프롬프트 템플릿 정의\n",
    "prompt_template = PromptTemplate(\n",
    "    input_variables=[\"noisy_text\"],\n",
    "    template=\"\"\"\n",
    "    다음은 노이즈가 포함된 텍스트입니다:\n",
    "    \"{noisy_text}\"\n",
    "    이 텍스트에서 의미를 유지하면서 노이즈를 제거하고, 자연스럽고 뉴스 제목처럼 간결하게 변환해줘.\n",
    "    다른 문구 말고 오직 변환된 텍스트만을 출력해줘.\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "# 결과를 저장할 리스트\n",
    "results = []\n",
    "\n",
    "# 배치 크기 설정\n",
    "batch_size = 10\n",
    "total_rows = len(data)\n",
    "\n",
    "# 기존 결과가 있으면 불러오기\n",
    "if os.path.exists(OUTPUT_FILE):\n",
    "    existing_results = pd.read_csv(OUTPUT_FILE)\n",
    "    processed_ids = set(existing_results['ID'])\n",
    "    results = existing_results.to_dict('records')\n",
    "    print(f\"기존에 {len(processed_ids)}개의 데이터를 처리했습니다. 이어서 진행합니다.\")\n",
    "else:\n",
    "    processed_ids = set()\n",
    "    print(\"처음부터 시작합니다.\")\n",
    "\n",
    "# 배치 처리\n",
    "for start_idx in range(0, total_rows, batch_size):\n",
    "    end_idx = min(start_idx + batch_size, total_rows)\n",
    "    batch = data.iloc[start_idx:end_idx]\n",
    "\n",
    "    for index, row in batch.iterrows():\n",
    "        id = row['ID']\n",
    "        if id in processed_ids:\n",
    "            continue  # 이미 처리된 데이터는 건너뜁니다\n",
    "\n",
    "        noisy_text = row['text']\n",
    "        target = row['target']\n",
    "\n",
    "        # 프롬프트 생성\n",
    "        prompt = prompt_template.format(noisy_text=noisy_text)\n",
    "\n",
    "        success = False\n",
    "        retry_count = 0\n",
    "        max_retries = 5\n",
    "\n",
    "        while not success and retry_count < max_retries:\n",
    "            try:\n",
    "                # 모델 예측\n",
    "                response = model.invoke(prompt)\n",
    "\n",
    "                # AIMessage 객체에서 실제 텍스트 추출\n",
    "                cleaned_text = response.content.strip()\n",
    "\n",
    "                # 결과 저장\n",
    "                results.append({\n",
    "                    \"ID\": id,\n",
    "                    \"Original Text\": noisy_text,\n",
    "                    \"Cleaned Text\": cleaned_text,\n",
    "                    \"target\": target\n",
    "                })\n",
    "\n",
    "                processed_ids.add(id)\n",
    "                print(f\"ID {id} 처리 완료.\")\n",
    "                success = True\n",
    "\n",
    "                # 요청 사이에 지연 시간 추가\n",
    "                time.sleep(2)  # 지연 시간을 늘려서 API 부하를 줄입니다\n",
    "\n",
    "            except Exception as e:\n",
    "                retry_count += 1\n",
    "                print(f\"ID {id} 처리 중 오류 발생: {e}\")\n",
    "                if 'ResourceExhausted' in str(e):\n",
    "                    wait_time = 30  # 쿼터 초과 시 1분 대기\n",
    "                    print(f\"{wait_time}초 후에 재시도합니다.\")\n",
    "                    time.sleep(wait_time)\n",
    "                else:\n",
    "                    print(f\"10초 후에 재시도합니다.\")\n",
    "                    time.sleep(10)\n",
    "\n",
    "        if not success:\n",
    "            print(f\"ID {id} 처리를 건너뜁니다.\")\n",
    "            # 실패한 경우에도 결과에 추가 (빈 문자열로)\n",
    "            results.append({\n",
    "                \"ID\": id,\n",
    "                \"Original Text\": noisy_text,\n",
    "                \"Cleaned Text\": \"\",\n",
    "                \"target\": target\n",
    "            })\n",
    "            processed_ids.add(id)\n",
    "\n",
    "    # 중간 저장\n",
    "    results_df = pd.DataFrame(results)\n",
    "    results_df.to_csv(OUTPUT_FILE, index=False, encoding='utf-8-sig')\n",
    "    print(f\"{end_idx}/{total_rows}개 데이터 처리 완료. 중간 결과를 저장했습니다.\")\n",
    "\n",
    "print(f\"모든 데이터 처리가 완료되었습니다. 결과는 '{OUTPUT_FILE}'에 저장되었습니다.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. postprocess_text_cleaning.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "text_cleaning.py를 통해 생성한 text_cleaned_raw.csv를 후처리하는 코드입니다.\n",
    "후처리 결과는 text_cleaned.csv, text_cleaned_nida.csv로 저장됩니다.\n",
    "\"\"\"\n",
    "\n",
    "INPUT_FILE = \"text_cleaned_raw.csv\"\n",
    "OUTPUT_FILE = \"text_cleaned.csv\"\n",
    "OUTPUT_NIDA_FILE = \"text_cleaned_nida.csv\"\n",
    "\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "df = pd.read_csv(INPUT_FILE)\n",
    "\n",
    "# 1. 컬럼 제거\n",
    "df.drop(columns=['Original Text'], inplace=True) # Original Text 컬럼 제거\n",
    "df.dropna(subset=['Cleaned Text'], inplace=True) # Cleaned Text 컬럼 결측치 제거\n",
    "\n",
    "\n",
    "# 2. 응답 오류 처리\n",
    "##  '->'이 포함된 문장에서 '->' 앞 문장 제거 ('문장 A -> 문장 B' ~> '문장 B')\n",
    "df['Cleaned Text'] = df['Cleaned Text'].apply(\n",
    "    lambda x: x.split('->', 1)[1].strip() if '->' in x else x\n",
    ")\n",
    "\n",
    "## '뉴스 제목:'이 포함된 문장에서 '뉴스 제목:' 앞 문장 제거\n",
    "df['Cleaned Text'] = df['Cleaned Text'].str.extract(\n",
    "    r'뉴스 제목:\\s*(.*)', expand=False\n",
    ").fillna(df['Cleaned Text'])\n",
    "\n",
    "## 답변에 \"니다.\"를 포함하는 행을 분리해 nida.csv에 저장\n",
    "contains_nida = df[df['Cleaned Text'].str.contains(\"니다.\", regex=False)]\n",
    "df = df[~df['Cleaned Text'].str.contains(\"니다.\", regex=False)]\n",
    "contains_nida.to_csv(OUTPUT_NIDA_FILE, index=False, encoding='utf-8-sig')\n",
    "\n",
    "\n",
    "# 3. 특수 문자 처리\n",
    "df['Cleaned Text'] = (\n",
    "    df['Cleaned Text']\n",
    "    .str.replace('\"', '', regex=False)\n",
    "    .str.replace(\"'\", '', regex=False)\n",
    "    .str.replace('#', '', regex=False)\n",
    "    .str.replace('*', '', regex=False)\n",
    "    .str.replace(', ', ' ', regex=False)\n",
    "    .str.replace('-', ' ', regex=False)\n",
    "    .str.replace('...', '…', regex=False)\n",
    "    .str.replace('….', '…', regex=False)\n",
    "    .str.replace(' · ', '·', regex=False)\n",
    "    .str.replace('· ', '·', regex=False)\n",
    "    .str.replace(' ·', '·', regex=False)\n",
    "    .str.replace('  ', ' ', regex=False)\n",
    "    .str.replace('  ', ' ', regex=False)\n",
    "    .str.strip()\n",
    ")\n",
    "\n",
    "# 4. 후처리 결과 저장\n",
    "df.rename(columns={'Cleaned Text': 'text'}, inplace=True) # 'Cleaned Text' 열 이름을 'text'로 변경\n",
    "df.sort_values(by='ID', ascending=True, inplace=True) # 'ID' 열을 기준으로 오름차순 정렬\n",
    "df.to_csv(OUTPUT_FILE, index=False, encoding='utf-8-sig') # CSV 파일로 저장\n",
    "\n",
    "print(f\"후처리가 완료되었습니다. 결과는 {OUTPUT_FILE}에 저장되었습니다.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Label Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. cleaning.py (태원 작성)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "cleanlab을 이용하여 filtering에서 생성한 label_noise.csv를 \n",
    "label_cleaned.csv로 cleaning하는 코드입니다.\n",
    "(label_noise 데이터는 postprocess 과정이 필요 없어 진행하지 않습니다.)\n",
    "\"\"\"\n",
    "\n",
    "INPUT_TEXT_CLEANED = \"text_cleaned.csv\"\n",
    "INPUT_LABEL_NOISE = \"../1_filtering/5_label_noise.csv\"\n",
    "OUTPUT_DATA = \"label_cleaned.csv\"\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, DataCollatorWithPadding\n",
    "from transformers import TrainingArguments, Trainer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "import os\n",
    "import evaluate\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "# 시드 설정\n",
    "SEED = 456\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "# 토크나이저 및 모델 로드\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"klue/bert-base\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"klue/bert-base\", num_labels=7)\n",
    "\n",
    "# GPU 사용 가능 여부 확인\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "BASE_DIR = os.getcwd()\n",
    "OUTPUT_DIR = os.path.join(BASE_DIR, '../outputs')\n",
    "\n",
    "# 데이터 로드\n",
    "clean_data = pd.read_csv(os.path.join(INPUT_TEXT_CLEANED))\n",
    "noise_data = pd.read_csv(os.path.join(INPUT_LABEL_NOISE))\n",
    "\n",
    "# 데이터셋 클래스 정의\n",
    "class BERTDataset(Dataset):\n",
    "    def __init__(self, data, tokenizer):\n",
    "        input_texts = data['text']\n",
    "        targets = data['target']\n",
    "        self.inputs = []; self.labels = []\n",
    "        for text, label in zip(input_texts, targets):\n",
    "            tokenized_input = tokenizer(text, padding='max_length', truncation=True, return_tensors='pt')\n",
    "            self.inputs.append(tokenized_input)\n",
    "            self.labels.append(torch.tensor(label))\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            'input_ids': self.inputs[idx]['input_ids'].squeeze(0),\n",
    "            'attention_mask': self.inputs[idx]['attention_mask'].squeeze(0),\n",
    "            'labels': self.labels[idx].squeeze(0)\n",
    "        }\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "# 데이터 분할 (clean 데이터만 사용)\n",
    "dataset_train, dataset_valid = train_test_split(clean_data, test_size=0.2, random_state=SEED)\n",
    "\n",
    "# 데이터셋 및 데이터로더 생성\n",
    "data_train = BERTDataset(dataset_train, tokenizer)\n",
    "data_valid = BERTDataset(dataset_valid, tokenizer)\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "# 평가 지표 설정\n",
    "f1 = evaluate.load('f1')\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    return f1.compute(predictions=predictions, references=labels, average='macro')\n",
    "\n",
    "# WandB 비활성화\n",
    "os.environ['WANDB_DISABLED'] = 'true'\n",
    "\n",
    "# 트레이닝 아규먼트 설정\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    overwrite_output_dir=True,\n",
    "    do_train=True,\n",
    "    do_eval=True,\n",
    "    logging_strategy='steps',\n",
    "    eval_strategy='steps',\n",
    "    save_strategy='steps',\n",
    "    logging_steps=100,\n",
    "    eval_steps=100,\n",
    "    save_steps=100,\n",
    "    save_total_limit=2,\n",
    "    learning_rate=2e-05,\n",
    "    per_device_train_batch_size=32,\n",
    "    per_device_eval_batch_size=32,\n",
    "    num_train_epochs=3,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model='eval_f1',\n",
    "    greater_is_better=True,\n",
    "    seed=SEED\n",
    ")\n",
    "\n",
    "# 트레이너 초기화\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=data_train,\n",
    "    eval_dataset=data_valid,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "# 모델 학습\n",
    "trainer.train()\n",
    "print(\"Fine-tuning 완료\")\n",
    "\n",
    "# 임베딩 추출 함수\n",
    "def get_embeddings(texts, model, tokenizer, device, batch_size=32):\n",
    "    model.eval()\n",
    "    model.config.output_hidden_states = True  # hidden states 활성화\n",
    "    embeddings = []\n",
    "    for i in tqdm(range(0, len(texts), batch_size), desc=\"텍스트 임베딩 중\"):\n",
    "        batch = texts[i:i+batch_size]\n",
    "        encoded_input = tokenizer(batch, padding=True, truncation=True, return_tensors='pt', max_length=256)\n",
    "        input_ids = encoded_input['input_ids'].to(device)\n",
    "        attention_mask = encoded_input['attention_mask'].to(device)\n",
    "        with torch.no_grad():\n",
    "            outputs = model(input_ids, attention_mask=attention_mask)\n",
    "            # 마지막 은닉층에서 [CLS] 토큰 벡터 추출\n",
    "            cls_embeddings = outputs.hidden_states[-1][:, 0, :].cpu().numpy()\n",
    "        embeddings.append(cls_embeddings)\n",
    "    # 임베딩 벡터 정규화 (L2 정규화)\n",
    "    normalized_embeddings = normalize(np.vstack(embeddings), norm='l2')\n",
    "    return normalized_embeddings\n",
    "\n",
    "# 클린 데이터와 노이즈 데이터의 임베딩 추출\n",
    "clean_embeddings = get_embeddings(clean_data['text'].tolist(), model, tokenizer, device)\n",
    "noise_embeddings = get_embeddings(noise_data['text'].tolist(), model, tokenizer, device)\n",
    "\n",
    "# RandomForest 분류기 학습 (전체 clean 데이터 사용)\n",
    "rf_classifier = RandomForestClassifier(n_estimators=100, random_state=SEED)\n",
    "rf_classifier.fit(clean_embeddings, clean_data['target'])\n",
    "\n",
    "# 노이즈 데이터에 대한 예측\n",
    "predicted_labels = rf_classifier.predict(noise_embeddings)\n",
    "\n",
    "# 노이즈 데이터의 라벨 교정\n",
    "noise_data['corrected_target'] = predicted_labels\n",
    "\n",
    "# 클린 데이터와 교정된 노이즈 데이터 합치기\n",
    "combined_data = noise_data[['ID', 'text', 'corrected_target']].rename(columns={'corrected_target': 'target'})\n",
    "\n",
    "# 결과 저장\n",
    "combined_data.to_csv(OUTPUT_DATA, index=False, columns=['ID', 'text', 'target'])\n",
    "print(f\"\\n교정된 데이터가 {OUTPUT_DATA}에 저장되었습니다.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. merge_cleaned.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "label_cleaned.csv와 text_cleaned.csv를 합쳐 train_cleaned.csv를 생성하는 코드입니다.\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "df_label = pd.read_csv('label_cleaned.csv')\n",
    "df_text = pd.read_csv('text_cleaned.csv')\n",
    "\n",
    "# 병합\n",
    "df_combined = pd.concat([df_label, df_text], ignore_index=True)\n",
    "\n",
    "# ID 기준으로 오름차순 정렬\n",
    "df_sorted = df_combined.sort_values(by='ID')\n",
    "\n",
    "# CSV로 저장\n",
    "df_sorted.to_csv('train_cleaned.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
